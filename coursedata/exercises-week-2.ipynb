{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b15595c9-1350-4425-a170-16c80185ac01",
   "metadata": {},
   "source": [
    "# **Exercise session 2: Convolutional neural network to detect NO$_2$ emissions from satellite data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb82752-d6ce-4e57-97ac-9c39a6af8586",
   "metadata": {},
   "source": [
    "In this exercise session you will train a convolutional neural network (CNN) to detect NO$_2$ plumes from Sentinel-5P TROPOMI instrument. This exercise is designed to be completed on Aalto JupyterHub. Please ensure that your notebook includes all necessary installation commands for any additional libraries your code requires. These commands should be clearly written and integrated within your notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5bb15c2f-df11-465f-9ae0-2c4732277a21",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import rasterio  # For handling raster data\n",
    "import geopandas as gpd  # For handling vector data\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a7c52f-33d7-4b75-8a86-9120dcece556",
   "metadata": {},
   "source": [
    "## **Exercise 2.1: Data preparation**\n",
    "\n",
    "In the previous exercise session, you built a small dataset of 20 images containing a plume and 20 images containing no plume. These images have been combined with those from your classmates to form a larger dataset, suitable for training a convolutional neural network (CNN). Your task now is to prepare this dataset for the training process.\n",
    "1. The pixel values in your images should be normalized to have values between 0 and 1. This is a common practice in image processing for neural networks, as it helps with the convergence during training.\n",
    "2. Separate the data into training, validation and test sets:\n",
    "   - **Training set:** Used for training the CNN. Typically, this is the largest portion of the data.\n",
    "   - **Validation set:** Used to tune the hyperparameters and evaluate the models during training.\n",
    "   - **Test set:** Used to test the model after the training process is complete. This set should only be used once to evaluate the final model performance.\n",
    "\n",
    "A common split ratio is 70% training, 15% validation, and 15% test. However, the exact split may depend on the size of your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa422bb7-4b64-4ded-81e9-e62249119d91",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory /notebooks/mlca2024/exercises-week-2\n",
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'coursedata'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 17\u001b[0m\n\u001b[1;32m     11\u001b[0m transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m     12\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor(),  \u001b[38;5;66;03m# Convert images to PyTorch tensors and scale to [0, 1]\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mNormalize((\u001b[38;5;241m0.5\u001b[39m,), (\u001b[38;5;241m0.5\u001b[39m,))  \u001b[38;5;66;03m# Normalize to have mean=0.5 and std=0.5 (adjust if necessary)\u001b[39;00m\n\u001b[1;32m     14\u001b[0m ])\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Load images using ImageFolder\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mImageFolder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcoursedata\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Splitting the dataset into train, validation, and test sets\u001b[39;00m\n\u001b[1;32m     21\u001b[0m train_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m0.7\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataset))\n",
      "File \u001b[0;32m/opt/software/lib/python3.10/site-packages/torchvision/datasets/folder.py:309\u001b[0m, in \u001b[0;36mImageFolder.__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    303\u001b[0m     root: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    307\u001b[0m     is_valid_file: Optional[Callable[[\u001b[38;5;28mstr\u001b[39m], \u001b[38;5;28mbool\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    308\u001b[0m ):\n\u001b[0;32m--> 309\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mIMG_EXTENSIONS\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_transform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_transform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_valid_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples\n",
      "File \u001b[0;32m/opt/software/lib/python3.10/site-packages/torchvision/datasets/folder.py:144\u001b[0m, in \u001b[0;36mDatasetFolder.__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    136\u001b[0m     root: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    141\u001b[0m     is_valid_file: Optional[Callable[[\u001b[38;5;28mstr\u001b[39m], \u001b[38;5;28mbool\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    142\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(root, transform\u001b[38;5;241m=\u001b[39mtransform, target_transform\u001b[38;5;241m=\u001b[39mtarget_transform)\n\u001b[0;32m--> 144\u001b[0m     classes, class_to_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_classes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m     samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_dataset(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot, class_to_idx, extensions, is_valid_file)\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader \u001b[38;5;241m=\u001b[39m loader\n",
      "File \u001b[0;32m/opt/software/lib/python3.10/site-packages/torchvision/datasets/folder.py:218\u001b[0m, in \u001b[0;36mDatasetFolder.find_classes\u001b[0;34m(self, directory)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_classes\u001b[39m(\u001b[38;5;28mself\u001b[39m, directory: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[List[\u001b[38;5;28mstr\u001b[39m], Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m]]:\n\u001b[1;32m    192\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Find the class folders in a dataset structured as follows::\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \n\u001b[1;32m    194\u001b[0m \u001b[38;5;124;03m        directory/\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;124;03m        (Tuple[List[str], Dict[str, int]]): List of all classes and dictionary mapping each class to an index.\u001b[39;00m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind_classes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/software/lib/python3.10/site-packages/torchvision/datasets/folder.py:40\u001b[0m, in \u001b[0;36mfind_classes\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_classes\u001b[39m(directory: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[List[\u001b[38;5;28mstr\u001b[39m], Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m]]:\n\u001b[1;32m     36\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Finds the class folders in a dataset.\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03m    See :class:`DatasetFolder` for details.\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m     classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(entry\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscandir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m entry\u001b[38;5;241m.\u001b[39mis_dir())\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m classes:\n\u001b[1;32m     42\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find any class folder in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdirectory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'coursedata'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "print(\"Current working directory\", os.getcwd())\n",
    "print()\n",
    "# Define the directories\n",
    "plume_dir = '../../../coursedata/plumes'\n",
    "n_plume_dir = '../../../coursedata/n-plumes'\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert images to PyTorch tensors and scale to [0, 1]\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize to have mean=0.5 and std=0.5 (adjust if necessary)\n",
    "])\n",
    "\n",
    "# Load images using ImageFolder\n",
    "dataset = datasets.ImageFolder(root='coursedata',\n",
    "                               transform=transform)\n",
    "\n",
    "# Splitting the dataset into train, validation, and test sets\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = int(0.15 * len(dataset))\n",
    "test_size = len(dataset) - (train_size + val_size)\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# Create DataLoaders for each set\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Check the sizes\n",
    "print(f\"Training set size: {len(train_dataset)}\")\n",
    "print(f\"Validation set size: {len(val_dataset)}\")\n",
    "print(f\"Test set size: {len(test_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d927da-69d9-400e-ab25-b35ad50ed75a",
   "metadata": {},
   "source": [
    "## **Exercise 2.2: Build a convolutional neural network model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8e4175-fc6c-4848-a6f3-7f0a0c68270a",
   "metadata": {},
   "source": [
    "Build a basic CNN model using a framework like TensorFlow or PyTorch. You can use [the code from this tutorial](https://www.tensorflow.org/tutorials/images/cnn) for constructing a simple CNN. The model doesn't need to be very deep; a few convolutional layers should be enough. Explain your model construction in your own words.\n",
    "\n",
    "Model structure guidelines (that you can ignore):\n",
    "1. **Input layer:** Your input layer should match the dimensions of your plume images.\n",
    "2. **Convolutional layers:**\n",
    "   - Start with 2-3 convolutional layers.\n",
    "   - Use small filters/kernels (like 3x3 or 5x5) to capture the image features.\n",
    "   - Apply activation functions like ReLU to introduce non-linearity.\n",
    "3. **Pooling layers:** After each convolutional layer, add a pooling layer (like max pooling) to reduce the spatial dimensions of the output volume.\n",
    "4. **Flatten layer:** Flatten the output from the convolutional layers before feeding it into the fully connected layers.\n",
    "5. **Fully connected (dense) layers:**\n",
    "   - A couple of dense layers at the end of the network for classification.\n",
    "   - The last dense layer should have the number of units equal to the number of classes and use a softmax activation function for multi-class classification.\n",
    "6. **Compile the model:**\n",
    "   - Compile the model with an appropriate optimizer (like Adam or SGD).\n",
    "   - Choose a suitable loss function (like binary_crossentropy for binary classification).\n",
    "   - Include accuracy as a metric for model performance evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bcd204-9093-4f03-9a9f-f721fdde1820",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d3f13568-870d-4bca-9f92-c19a670bed80",
   "metadata": {},
   "source": [
    "## **Exercise 2.3: Train the model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0bfe57-eb4b-44d3-a8d4-4acc08e60eb8",
   "metadata": {},
   "source": [
    "1. Use the training and validation data to train the model.\n",
    "2. Monitor the loss and accuracy metrics over epochs to observe the training process.\n",
    "3. Save the model (optional): After training, you can save your model for future use.\n",
    "\n",
    "Things to Monitor:\n",
    "- **Overfitting:** If your model performs well on training data but poorly on validation data, it might be overfitting. Consider using dropout layers or getting more data.\n",
    "- **Underfitting:** If the model performs poorly on both training and validation data, it might be underfitting. Consider adding complexity to your model or training for more epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeec2215-8259-4e76-858a-ab55d417e283",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "03269f9e-7e9e-4a1a-8abf-a8bdd79e9b76",
   "metadata": {},
   "source": [
    "## **Exercise 2.4: Model evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6e9246-fd78-4750-92e4-7d4cd91e53a9",
   "metadata": {},
   "source": [
    "After training your model, the next step is to evaluate its performance using the test set. This step is crucial to understand how well your model will perform on unseen data. Some common metrics to evaluate a CNN that performs a classification task are\n",
    "- **Accuracy:** Measures the percentage of correctly predicted instances out of all predictions. It's straightforward but can be misleading in cases of class imbalance.\n",
    "- **Precision** (Positive predictive value): The ratio of correctly predicted positive observations to the total predicted positives. It's important when the cost of False Positives is high.\n",
    "- **Recall** (sensitivity, true positive rate): The ratio of correctly predicted positive observations to all actual positives. Crucial when the cost of false negatives is high.\n",
    "- **F1 score:** The weighted average of precision and recall. Useful as a single metric when you need a balance between precision and recall.\n",
    "- **Confusion matrix:** A table layout that allows visualization of the performance of an algorithm. Shows the number of true positives, false positives, true negatives, and false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a71e96-3393-42d5-8a46-21db35fda63f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "614b8150-39f5-402f-98a4-d61f5a6e05f4",
   "metadata": {},
   "source": [
    "## **Exercise 2.5: Visualization of results**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adf9365-5063-4908-95a4-bc8719bc8bb2",
   "metadata": {},
   "source": [
    "Visualize model predictions: Choose a subset of your test dataset and display the images with their predicted and actual labels. This is particularly useful to see how the model performs on individual examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6d385d-a9e1-4828-ad43-1f3618aef77a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9eaaea4a-5543-41aa-afee-35aaa29cd035",
   "metadata": {},
   "source": [
    "## **Exercise 2.6: Test your model on the real world** (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fe9664-f2c2-479d-97d8-f856f688b346",
   "metadata": {},
   "source": [
    "1. Pick an area of roughly 500 000 km$^2$ that includes industrial activity, and consider a period of around one month to ensure enough data. Use Copernicus Browser for a preliminar analysis of a good area.\n",
    "2. Retrieve Sentinel-5P NO$_2$ data from that area and that time period.\n",
    "3. Prepare the data to fit your CNN. The images you feed your CNN need to be of size 64x64 pixels. Make sure you retain the coordinates of your data to be able to locate potential power plants or cities later.\n",
    "4. Look for plumes using your trained CNN.\n",
    "5. How many plumes can you find? Can you link them to any known city or power plant?\n",
    "\n",
    "**Note:** You will need to identify cloud covered-areas and exclude them from the dataset. You can also interpolate missing values due to clouds if there are only a few missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fc254e-520f-4c10-bd2f-8af9bbef0130",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "498cecc0-ccfe-4f7f-8e0f-d3c5be93d748",
   "metadata": {},
   "source": [
    "## **Exercise 2.7: Overview**\n",
    "\n",
    "Reflect on the entire process you've undertaken, from the initial data collection to building and training your machine learning algorithm, and finally evaluating this algorithm in a real-world setting. Which step did you find the most challenging, and why? Additionally, were there any unexpected difficulties or learning experiences along the way? How do you think these challenges could be addressed in future projects? (Imagine this was a task in your job, this is not a question on how to improve homework exercises!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bf8958-1157-4a40-b86e-7a57410c5b32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8abda1d4-1b87-4271-b023-0f6541c7f127",
   "metadata": {},
   "source": [
    "## **Exercise 2.8: Comparison**\n",
    "Compare your approach for detecting NO$_2$ plumes with the method described in the class-reviewed paper [Automated detection of atmospheric NO$_2$ plumes from satellite data: a tool to help infer anthropogenic combustion emissions.](https://amt.copernicus.org/articles/15/721/2022/). Focus on key differences and similarities in the data processing techniques, CNN architecture, and overall detection strategy. Briefly discuss how these differences might impact the effectiveness of plume detection\n",
    "\n",
    "Discuss how using a plume detection algorithm based on a CNN model can be utilized for climate action. Interpret the results, noting any interesting observations, and suggest potential areas for model improvement. Could other approaches be better for such a task? What steps would you take next?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef8b556-2214-4599-9ea2-5cd92dcd4c27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
